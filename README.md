# Adaptive RAG Chatbot

A fully offline, intelligent document Q&A system with adaptive response capabilities.

## ğŸš€ Features

- **ğŸ§  Adaptive Response System**: Automatically adjusts answer length based on question complexity
- **ğŸ” Dual-Source Architecture**: Search both company documents and temporary uploads
- **âš¡ Speed Optimized**: Fast responses with intelligent caching and optimized parameters
- **ğŸŒ Fully Offline**: Uses local Ollama models - no internet required
- **ğŸ“Š ChromaDB Vector Storage**: Persistent vector database for semantic search
- **ğŸ¨ Enhanced UI**: Visual indicators showing response type (Short/Medium/Detailed)

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit
- **Vector Database**: ChromaDB
- **Language Model**: llama3.2:1b (via Ollama)
- **Embedding Model**: nomic-embed-text (via Ollama)
- **Document Processing**: LangChain + PyMuPDF
- **Architecture**: RAG (Retrieval-Augmented Generation)

## ğŸ“‹ Prerequisites

1. **Python 3.11+**
2. **Ollama** installed and running
3. Required Ollama models:
   ```bash
   ollama pull llama3.2:1b
   ollama pull nomic-embed-text
   ```

## ğŸš€ Quick Start

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd BrownieAgent
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # or
   source .venv/bin/activate  # Linux/Mac
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama service**
   ```bash
   ollama serve
   ```

5. **Run the application**
   ```bash
   streamlit run app.py
   ```

6. **Open browser** to `http://localhost:8501`

## ğŸ“ Project Structure

```
BrownieAgent/
â”œâ”€â”€ app.py                    # Main adaptive RAG application
â”œâ”€â”€ app_simple.py            # Simple version of the app
â”œâ”€â”€ get_embedding_function.py # Embedding configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ data/                   # Put your PDF documents here
â”œâ”€â”€ chroma_vector_db/       # ChromaDB storage (auto-created)
â””â”€â”€ test_*.py              # Test files
```

## ğŸ’¡ How It Works

### Adaptive Response System
The system analyzes your questions and automatically provides:
- **ğŸŸ¢ Short Answers** (200 tokens): For simple questions like "What is X?"
- **ğŸŸ¡ Medium Answers** (350 tokens): For general questions about features, benefits
- **ğŸ”µ Detailed Answers** (500 tokens): For complex questions requiring explanation

### Document Sources
- **Company Documents**: Place PDFs in the `data/` folder for permanent access
- **Temporary Documents**: Upload PDFs through the UI for one-time queries

## ğŸ”§ Configuration

### Model Configuration
Edit `get_embedding_function.py` to change:
- Embedding model settings
- Timeout configurations
- Custom model paths

### Response Tuning
Edit `app.py` to adjust:
- Token limits for each response type
- Temperature settings
- Question analysis keywords

## ğŸ“Š Performance

- **Response Time**: 2-5 seconds per query
- **Document Processing**: ~1 second per page
- **Memory Usage**: ~2GB with loaded models
- **Offline Operation**: 100% - no internet required

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“ License

[Add your license here]

## ğŸ™ Acknowledgments

- Ollama for local LLM hosting
- ChromaDB for vector storage
- LangChain for document processing
- Streamlit for the web interface
